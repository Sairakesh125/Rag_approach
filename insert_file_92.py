import os
import numpy as np
from pathlib import Path
from dotenv import load_dotenv
from PyPDF2 import PdfReader
from sentence_transformers import SentenceTransformer
from pinecone import Pinecone
from keybert import KeyBERT
from transformers import pipeline

# ====== Load .env credentials ======
load_dotenv()
pc = Pinecone(api_key=os.getenv("PINECONE_API_KEY"))
index = pc.Index(os.getenv("PINECONE_INDEX_NAME"))
namespace = "__default__"

# ====== Load Models ======
embedding_model = SentenceTransformer("all-mpnet-base-v2")
kw_model = KeyBERT()
ner_model = pipeline("ner", model="Jean-Baptiste/roberta-large-ner-english", aggregation_strategy="simple")
summarizer = pipeline("summarization", model="facebook/bart-large-cnn")

# ====== Hardcoded PDF Path (92nd file) ======
pdf_path = r"C:\Users\naape\Downloads\Rag_dense_project\data\MHC_CaseStatus_511692.pdf"

# ====== Step 1: Chunk PDF ======
def read_pdf_chunks(pdf_path, max_chunk_len=500):
    reader = PdfReader(pdf_path)
    full_text = "\n".join(
        page.extract_text() for page in reader.pages if page.extract_text()
    )
    words = full_text.split()
    chunks = [
        " ".join(words[i:i + max_chunk_len]) for i in range(0, len(words), max_chunk_len)
    ]
    return chunks, full_text

# ====== Step 2: Metadata Extraction ======
def extract_metadata(full_text):
    keywords = [kw[0] for kw in kw_model.extract_keywords(full_text, top_n=10)]
    intent = "judgement_summary" if "judgement" in full_text.lower() else "legal_case"
    entities = ner_model(full_text[:4000])
    named_entities = [f"{e['word']} ({e['entity_group']})" for e in entities]
    summary = summarizer(full_text[:1024])[0]["summary_text"]

    return {
        "intent": intent,
        "keywords": keywords,
        "named_entities": named_entities,
        "summary": summary
    }

# ====== Step 3: Embedding + Upload with Safety Checks ======
def embed_and_upload(doc_id, chunks, metadata):
    embeddings = embedding_model.encode(chunks)
    vector_avg = [float(x) for x in np.mean(embeddings, axis=0)]

    if len(vector_avg) != 768:
        print(f"‚ùå Vector dimension mismatch: expected 768, got {len(vector_avg)}")
        return

    if any(np.isnan(x) or np.isinf(x) for x in vector_avg):
        print(f"‚ùå Invalid vector: NaN or Inf found for {doc_id}")
        return

    try:
        response = index.upsert(
            vectors=[{
                "id": doc_id,
                "values": vector_avg,
                "metadata": {
                    "filename": f"{doc_id}.pdf",
                    **metadata
                }
            }],
            namespace=namespace
        )
        print(f"‚úÖ Upserted vector with ID: {doc_id}")
    except Exception as e:
        print(f"‚ùå Upsert failed for {doc_id}: {e}")

# ====== Main ======
def main():
    doc_id = Path(pdf_path).stem

    if not Path(pdf_path).exists():
        print(f"‚ùå File not found: {pdf_path}")
        return

    print(f"üìÑ Processing file: {pdf_path}")
    chunks, full_text = read_pdf_chunks(pdf_path)
    metadata = extract_metadata(full_text)
    embed_and_upload(doc_id, chunks, metadata)
    print("üéâ Insert process completed.")

if __name__ == "__main__":
    main()
